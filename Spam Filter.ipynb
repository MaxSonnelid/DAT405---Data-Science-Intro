{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Keywords: advanced imports, word count, Naive Bayes Classifier, data cleaning, regular expressions, NLP (attempt)"
   ]
  },
  {
   "source": [
    "1a - Preprossesing "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Ham_type = 'Hard_ham'\n",
    "\n",
    "# Creating a method that reads the files\n",
    "def read_file(file):\n",
    "    with open(file, \"r\", encoding=\"Latin-1\") as f:\n",
    "        return f.read()\n",
    "\n",
    "# Creating df for ham emails\n",
    "df_ham = pd.DataFrame([read_file(file) for file in ['/Users/ericjohansson/Dropbox/Skolan/Introduction to Data Science and AI/Assignments/Assignment-4/notes/' + Ham_type + \"/\" + path for path in os.listdir('/Users/ericjohansson/Dropbox/Skolan/Introduction to Data Science and AI/Assignments/Assignment-4/notes/' + Ham_type)]])\n",
    "\n",
    "# Creating df for spam emails\n",
    "df_spam = pd.DataFrame([read_file(file) for file in ['/Users/ericjohansson/Dropbox/Skolan/Introduction to Data Science and AI/Assignments/Assignment-4/notes/spam' +  \"/\" + path for path in os.listdir('/Users/ericjohansson/Dropbox/Skolan/Introduction to Data Science and AI/Assignments/Assignment-4/notes/spam')]])"
   ]
  },
  {
   "source": [
    "1b - Splitting the data "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_ham['labels'] = 0\n",
    "df_spam['labels'] = 1\n",
    "df_ham.columns = ['messages','labels']\n",
    "df_spam.columns = ['messages', 'labels']\n",
    "total_emails = df_ham.append(df_spam, ignore_index=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_emails['messages'],total_emails['labels'], test_size = .35, random_state = 42)\n",
    "\n",
    "#total_emails['messages'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(488, 58741)\n(263, 58741)\n"
    }
   ],
   "source": [
    "# Using countvectorizer to count the words in X_train and X_test\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "\n",
    "X_train_fit = cv.fit_transform(X_train)\n",
    "X_test_fit = cv.transform(X_test)\n",
    "\n",
    "print(X_train_fit.shape)\n",
    "print(X_test_fit.shape)"
   ]
  },
  {
   "source": [
    "Naive Bayes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BernoulliNB(binarize=True)\n\n0.7984790874524715\n"
    }
   ],
   "source": [
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BernNB = BernoulliNB(binarize = True)\n",
    "BernNB.fit(X_train_fit, y_train)\n",
    "print(BernNB)\n",
    "print()\n",
    "\n",
    "y_expect = y_test\n",
    "y_pred = BernNB.predict(X_test_fit)\n",
    "print(accuracy_score(y_expect, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MultinomialNB()\n\n0.9315589353612167\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MultiNB = MultinomialNB()\n",
    "MultiNB.fit(X_train_fit, y_train)\n",
    "print(MultiNB)\n",
    "print()\n",
    "\n",
    "y_expect = y_test\n",
    "y_pred = MultiNB.predict(X_test_fit)\n",
    "print(accuracy_score(y_expect, y_pred))"
   ]
  },
  {
   "source": [
    "## Task 3 "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Was done by switching Ham_type to 'Hard_ham'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Task 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a method that deletes html\n",
    "\n",
    "import re\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('(<script(\\s|\\S)*?<\\/script>)|(<style(\\s|\\S)*?<\\/style>)|(<!--(\\s|\\S)*?-->)|(<\\/?(\\s|\\S)*?>)')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)  \n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a method that deletes the header in a mail\n",
    "\n",
    "def split_email(email):\n",
    "    temp = email.split('\\n\\n')\n",
    "    res = \"\"\n",
    "    for i in range (1, len(temp)):\n",
    "        res += temp[i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting html and the headers in total_emails\n",
    "for i in (range(total_emails.shape[0])):\n",
    "    total_emails['messages'][i] = (split_email(cleanhtml(total_emails['messages'][i])))\n",
    "\n",
    "#total_emails['messages'][15]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test-printing one email to see that both the header and the HTML are deleted.\n",
    "#print(total_emails['messages'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_emails['messages'],total_emails['labels'], test_size = .5, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "X_train:  (375, 3426)\nX_test:  (376, 3426)\n"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', token_pattern='[\\S]+', min_df=5)\n",
    "\n",
    "#training set\n",
    "X_train_fit = cv.fit_transform(X_train)\n",
    "print(\"X_train: \", X_train_fit.shape)\n",
    "\n",
    "#testing set\n",
    "X_test_fit = cv.transform(X_test)\n",
    "print(\"X_test: \", X_test_fit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "BernoulliNB(binarize=True)\n0.851063829787234\n"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "BernNB = BernoulliNB(binarize = True)\n",
    "BernNB.fit(X_train_fit, y_train)\n",
    "print(BernNB)\n",
    "\n",
    "y_expect = y_test\n",
    "y_pred = BernNB.predict(X_test_fit)\n",
    "print(accuracy_score(y_expect, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MultinomialNB()\n\n0.9175531914893617\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MultiNB = MultinomialNB()\n",
    "MultiNB.fit(X_train_fit, y_train)\n",
    "print(MultiNB)\n",
    "print()\n",
    "\n",
    "y_expect = y_test\n",
    "y_pred = MultiNB.predict(X_test_fit)\n",
    "print(accuracy_score(y_expect, y_pred))"
   ]
  },
  {
   "source": [
    "## Extra (Things we would like to look more into if we had more time)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 714
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download() ## wordnet identifier was downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(word) >>> caress\n(word) >>> fli\n(word) >>> die\n(word) >>> mule\n(word) >>> deni\n(word) >>> die\n(word) >>> agre\n(word) >>> own\n(word) >>> humbl\n(word) >>> size\n"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died', 'agreed', 'owned', 'humbled', 'sized']\n",
    "\n",
    "for word in plurals:\n",
    "    print (f\"(word) >>> {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(cv.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
