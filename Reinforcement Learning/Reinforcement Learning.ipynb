{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Z8bJh0mZtn"
      },
      "source": [
        "# **Assignment 5**\n",
        "***Eric Johansson & Max Sonnelid***\n",
        "\n",
        "Hours spent: 15 hours per person "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Key words: Reinforcement learning, optimal path algorithm, value iteration, optimal policy, Q-learning, AI gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go-n7fCbTh-s"
      },
      "source": [
        "# Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHaFoaJaWBnt"
      },
      "source": [
        "1a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjUR1-J2WgWU"
      },
      "source": [
        "Optimal path is either: **EENNN** or **EENNWNE**. Since there are two different paths, there does not exist an unique solution. However, if $\\gamma$ < 1 (the discounting rate is less than zero), the first path (EENNN) would uniquely be the optimum as the optimum is reached faster than in the second alternative.\n",
        "\n",
        "Both of these paths will result a **total reward** of 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgkveEKGWPju"
      },
      "source": [
        "1b)\n",
        "\n",
        "We define all the states as the following matrix, where (0,0) is the starting state and (2,2) is the exit state.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "[(0,3) (1,3) (2,3)\n",
        "(0,2) (1,2) (2,2)\n",
        "(0,1) (1,1) (2,1)\n",
        "(0,0) (1,0) (2,0)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej8vuJVaZGNG"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Policy for each state:\n",
        "\n",
        "\n",
        "(0,0) = E \n",
        "\n",
        "(0,1) = E or N\n",
        "\n",
        "(0,2) = E or N\n",
        "\n",
        "(0,3) = E or N\n",
        "\n",
        "(1,0) = E\n",
        "\n",
        "(1,1) = E or N\n",
        "\n",
        "(1,2) = E or N\n",
        "\n",
        "(1,3) = S or E\n",
        "\n",
        "(2,0) = N\n",
        "\n",
        "(2,1) = N\n",
        "\n",
        "(2,2) = N or E\n",
        "\n",
        "\n",
        "By following this policy, an optimal solution will be reached regardless of what start point that is selected. Note that optimal value can differ depending on what point that is selected as starting point\n",
        "\n",
        "What is intersting to note is that the optimal policy can be reach with fewer iterations compared with optimal values. Therefore, if only the optimal policy is required, a lot of computing power could be saved by only focusing on optimal policy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe-ztkJ5WY7U"
      },
      "source": [
        "1c)\n",
        "\n",
        "If starting point is set to (0,0) the expected reward with optimal solution is 0. However, the expected reward is different among the starting points, for example (1,0) will result in a optimal solution 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL70g4IGTsIg"
      },
      "source": [
        "# Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2LYtXukk2Bf"
      },
      "source": [
        "2a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnLrQ1Grb-5X",
        "outputId": "72074f2b-63a3-4ae3-e5f8-4b3e3f87915c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Create the reward matrix\n",
        "import numpy as np\n",
        "matrix =  np.arange(2.0, 11.0).reshape(3,3)\n",
        "#print(x)\n",
        "\n",
        "for i in range(0,3):\n",
        "    for j in range (0,3):\n",
        "        matrix[i][j] = 0.0\n",
        "\n",
        "\n",
        "matrix[1][1]=10.0\n",
        "print(matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.  0.  0.]\n",
            " [ 0. 10.  0.]\n",
            " [ 0.  0.  0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL9EvVJ1b_dl"
      },
      "source": [
        "#Method for checking whether a step is feasible or not\n",
        "#If step is not feasible, return 0\n",
        "def testStep(matrix, x, y, dir):\n",
        "    if dir == 'N':\n",
        "        if x-1<0:\n",
        "            return 0\n",
        "        else: return matrix[x-1][y]\n",
        "    if dir == 'S':\n",
        "        if x+1>2:\n",
        "            return 0\n",
        "        else: return matrix[x+1][y]\n",
        "    if dir == 'E':\n",
        "        if y+1>2:\n",
        "            return 0\n",
        "        else: return matrix[x][y+1]\n",
        "    if dir == 'W':\n",
        "        if y-1<0:\n",
        "            return 0\n",
        "        else: return matrix[x][y-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0kYnddicCNc"
      },
      "source": [
        "#Calculate the expected reward for each possible action in a certain state\n",
        "#This is based both on the potential current reward and the discounted historical potential reward\n",
        "#The highest expected reward for any of the possible actions is returned\n",
        "def maxValue_new(x, y, original, updated):\n",
        "    N = 0.8*(testStep(original,x,y,'N')+0.9*testStep(updated,x,y,'N'))+0.2*(original[x][y]+0.9*updated[x][y])\n",
        "    S = 0.8*(testStep(original,x,y,'S')+0.9*testStep(updated,x,y,'S'))+0.2*(original[x][y]+0.9*updated[x][y])\n",
        "    E = 0.8*(testStep(original,x,y,'E')+0.9*testStep(updated,x,y,'E'))+0.2*(original[x][y]+0.9*updated[x][y])\n",
        "    W = 0.8*(testStep(original,x,y,'W')+0.9*testStep(updated,x,y,'W'))+0.2*(original[x][y]+0.9*updated[x][y])\n",
        "    return (max(N,S,E,W))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8ljkzIxcFTD",
        "outputId": "ac9a1578-28b3-4d62-c25e-8e615dffb15b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#An 3x3 matrix filled with only 0 is created\n",
        "def empty_matrix():\n",
        "    res =  np.arange(2.0, 11.0).reshape(3,3)\n",
        "\n",
        "    for i in range(0,3):\n",
        "         for j in range (0,3):\n",
        "              res[i][j] = 0.0\n",
        "    return res\n",
        "\n",
        "print(empty_matrix())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOn98jkedF87",
        "outputId": "98d88b2c-f803-4733-a823-b0c4305c5359",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "MI6 =  np.arange(73.0, 82.0).reshape(3,3)\n",
        "MI6.astype(float)\n",
        "print(M16)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[73. 74. 75.]\n",
            " [76. 77. 78.]\n",
            " [79. 80. 81.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN7cji3hcI3G",
        "outputId": "e36a2d74-de35-47a0-c03c-917ba7ae234b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#Method for performing value iteration based on the problem described in question 2 in the assignment\n",
        "def iterate(nr, start_matrix):\n",
        "    temp = empty_matrix() #Start with an empty matrix\n",
        "\n",
        "    for i in range(0,3):\n",
        "        for j in range (0,3):\n",
        "            #Based on the start_matrix (reward matrix), calculate the expected reward for each state in one iteration\n",
        "            #Replace empty_matrix() with MI6 if you want to have initial value other than zero\n",
        "            temp[i][j] = maxValue_new(i, j, start_matrix, empty_matrix())\n",
        "    if (nr > 1):\n",
        "        for i in range(nr-1):  #Number of iterations for calculating the reward matrix\n",
        "            temp2 = empty_matrix()\n",
        "            for i in range(0,3):\n",
        "                 for j in range (0,3):\n",
        "                      #Based on the start_matrix (reward matrix) and temp (expected reward matrix from last iteration), calculate the expected reward for each state\n",
        "                      temp2[i][j] = maxValue_new(i, j, start_matrix, temp) \n",
        "            temp = temp2\n",
        "    return temp #Return the reward matrix calculated based on n iterations\n",
        "\n",
        "print(iterate(14000,matrix))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[45.61292366 51.94805195 45.61292366]\n",
            " [51.94805195 48.05194805 51.94805195]\n",
            " [45.61292366 51.94805195 45.61292366]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnui3IqzYzJC"
      },
      "source": [
        "The value function for each state will converge to the value as shown in the picture below. The optimal policy for each state is also shown in the picture as arrows pointing in the direction with the highest expected reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkjtnBileK0n",
        "outputId": "6ad61c82-bda6-4c38-f1a5-c7fecfaea77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "Image(url= \"https://i.postimg.cc/SQ1F9tPv/optimal-policy.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.postimg.cc/SQ1F9tPv/optimal-policy.png\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPq_hlsVk5Lx"
      },
      "source": [
        "2b)\n",
        "\n",
        "No matter what inital values are set, the optimal policy will still be the same. However, if the initial matrix contains big numbers, more iterations will be needed in order for the matrix to converge. This is due to the discount rate with after each iteration takes the historical rewards values less into account. The oldest historical rewards values, namely the initial values, is therefore taken into account the least. As one single algorithm is used for all iterations, the effect from the initial value will be negligible after a big number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9IysutGTsmE"
      },
      "source": [
        "# Question 3\n",
        "\n",
        "In this question, a Q-table for the 'NChain-v0' environment will be created based on the code given for the 'Frozen Lake' environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fY_I7hQx_MVH",
        "outputId": "60e47b89-adcb-473c-a355-0c8ae7715175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "pip install gym"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyV1cXYj_Pul"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dNt8WGO_R6j"
      },
      "source": [
        "#Create the 'NChain-v0' environment\n",
        "env = gym.make('NChain-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwJXK5PP_UyY",
        "outputId": "6ed43013-3c02-48b4-b346-9978ce40cfba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#Print the size of the action resp. state space\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Action Space Discrete(2)\n",
            "State Space Discrete(5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlLbaFsF_ZAR",
        "outputId": "4fa292b3-b3d7-4809-ba69-e43da339da8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# Parameters for the Q-learning algorithm\n",
        "num_episodes = 500 #20000 #60000\n",
        "gamma = 0.95 #0.99\n",
        "learning_rate = 0.1 #0.95 #0.85\n",
        "epsilon = 0.5 #1 #0.15 #0.1\n",
        "\n",
        "# initialize the Q table\n",
        "Q = np.zeros([5, 2])\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.],\n",
              "       [0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK6dRE6Y_bgp"
      },
      "source": [
        "#Q-learning model taken from the 'Frozen Lake' example\n",
        "for _ in range(num_episodes):\n",
        "\tstate = env.reset()\n",
        "\tdone = False\n",
        "\twhile done == False:\n",
        "        # First we select an action:\n",
        "\t\tif random.uniform(0, 1) < epsilon: # Flip a skewed coin\n",
        "\t\t\taction = env.action_space.sample() # Explore action space\n",
        "\t\telse:\n",
        "\t\t\taction = np.argmax(Q[state,:]) # Exploit learned values\n",
        "        # Then we perform the action and receive the feedback from the environment\n",
        "\t\tnew_state, reward, done, info = env.step(action)\n",
        "        # Finally we learn from the experience by updating the Q-value of the selected action\n",
        "\t\tupdate = reward + (gamma*np.max(Q[new_state,:])) - Q[state, action]\n",
        "\t\tQ[state,action] += learning_rate*update \n",
        "\t\tstate = new_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWZxOX-4_cWw",
        "outputId": "b12b6615-5a8c-475e-f2ca-537b1eba1cb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "#Printing the Q table\n",
        "Q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[61.17578161, 59.9919813 ],\n",
              "       [64.53830675, 60.96944918],\n",
              "       [69.13579502, 62.10568061],\n",
              "       [72.38577523, 64.33071915],\n",
              "       [81.97569491, 69.29297027]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWRGkRiyFVsp"
      },
      "source": [
        "# Question 4\n",
        "\n",
        "4a)\n",
        "\n",
        "Below you will find an illustration for the Markov Decision Process for the 'NChain-v0' environment (copied from the assignment).\n",
        "\n",
        "![alt text](https://i.postimg.cc/rwMKQdSk/chain-mdp.png)\n",
        "\n",
        "Translating this illustration into code is based on the code for the value iteration in question 2. One major difference is that the rewards in this environment is much stronger connected to the transition between states compared to the environment in question 2. Therefore, we chose to not start with a reward matrix, but rather to give a reward of 2 if the backward action (b) is successful, 0 if the forward action (f) is successful in state 0, 1, 2 & 3, and 10 if the forward action (f) is successful in state 4. This is made in the method ```maxValue_new```. Small adaptations were also made as this observation space is best described by a 1x5 matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFyaRD_aFa11"
      },
      "source": [
        "#Calculating the expected reward for each action in a certain state based on the given reward distribution and the historical rewards given (upd).\n",
        "#The highest expected reward for any action is returned.\n",
        "def maxValue_new(s, upd):\n",
        "    if (s == 4):\n",
        "        f = 0.8*(10+0.95*upd[s])+0.2*(2+0.95*upd[0])\n",
        "        b = 0.8*(2+0.95*upd[0])+0.2*(10+0.95*upd[s])\n",
        "    else:\n",
        "        f = 0.8*(0.95*upd[s+1])+0.2*(2+0.95*upd[0])\n",
        "        b = 0.8*(2+0.95*upd[0])+0.2*(0.95*upd[s+1])\n",
        "    return max(f, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EUEnoKGFiPn",
        "outputId": "0d47c4a5-2e6e-4551-fc39-11254d8cdf92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def iterate(nr):\n",
        "    temp = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "    \n",
        "    #An inital value iteration is made\n",
        "    for i in range(0,5):\n",
        "        temp[i] = maxValue_new(i, [0.0, 0.0, 0.0, 0.0, 0.0]) #Highest expected reward for each state is calculated with no historical rewards given (upd is zero).\n",
        "\n",
        "    #Additional value iterations are mode\n",
        "    if (nr > 1):\n",
        "        for i in range(nr-1):\n",
        "            temp2 = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "            for i in range(0,5):\n",
        "                temp2[i] = maxValue_new(i, temp) #Highest expected reward for each state is calculated based on historical rewards calculated in the last iteration\n",
        "            temp = temp2\n",
        "    return temp #Return the reward matrix calculated based on n iterations\n",
        "\n",
        "print(iterate(500))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[61.37948159948314, 64.89128959948314, 69.51208959948313, 75.59208959948315, 83.59208959948315]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWddU4meGVl6"
      },
      "source": [
        "Comparing the Q-table at the end of question 3 with the table for the optimal value function just above, gives results that almost exactly follows the relationship between $Q^*(s,a)$ and $V^*(s)$ as stated in the assignment:\n",
        "\n",
        "$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i90jKhNJkexh"
      },
      "source": [
        "Below, the result of both the q-learning algoritm as well as value iteration algoritm is displayed. As expected the values are almost identicial. However, there is a slight difference between the values in each state. This could partly depend on the exploration rate that exist in the q-learning method (which is currently split evenly between exploration and exploatation). Moreover, the low learning rate requires a large amount of iterations for the values to converge. \n",
        "\n",
        "| | | |  \n",
        "|----------|----------|---------|  \n",
        "|61.2|60.0|\n",
        "|64.5|61.0|  \n",
        "|69.1|62.1|  \n",
        "|72.4|64.3|\n",
        "|82.0|69.3| \n",
        "\n",
        "| | | \n",
        "|----------|----------|\n",
        "|61.4|\n",
        "|64.9|\n",
        "|69.5|\n",
        "|75.6|\n",
        "|83.6|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0QlLWUHD50C"
      },
      "source": [
        "4b)\n",
        "\n",
        "Running a Reinforcement Learning model without any exploration could be called a greedy policy, in which the agent always takes the action with the highest expected value. An everyday example could be when a person (Mr. Smith) decides what restaurant too choose. With a greedy policy, Mr. Smith would decide for the restaurant which has given Mr. Smith the highest satisfaction in the past.\n",
        "However, with such a greedy policy Mr. Smith risks missing out of other restaurants which could bring him an even higher satisfaction than what he has experienced in the past. As Mr. Smith has not visited all restaurants, there are many restaurants with an unknown expected satisfaction value, which he is not able choose with a greedy policy.\n",
        "\n",
        "Instead, Mr. Smith could do a so called epsilon-greedy policy, where he uses the greedy policy with a probability of 1-epsilon and takes a random choice with a probability of epsilon. With this epsilon-greedy policy, Mr. Smith will eventually visit all restaurants and thus find out about the satisfaction value for each restaurant. Mr. Smith of course risks choosing a restaurant with a low satisfaction value when doing a random choice. As he uses the greedy policy a certain share of the decision, he will be certain to get a high satisfaction these times.\n",
        "\n",
        "Described more abstractly, with a greedy policy the RL model risks not exploring the whole action space as it is only making decisions based on previous action. The solution to this is to use a epsilon-greedy-policy, which some times makes decisions according to the greedy policy and some times make decisions on random with the aim to find even higher rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66NzS8xIDZyv"
      },
      "source": [
        "#Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtCKc9VHETIi"
      },
      "source": [
        "5a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWzupBXZEW_K"
      },
      "source": [
        "**Decision trees**\n",
        "\n",
        "A decision tree is a way of defining something based on a number of binary separations. A decision tree consist of the root, nodes, and leafs (see picture below). To build a tree, data from a dataset consisting of multiple features and one target is needed. The target is the question that is supposed to be answered by using a decision tree, e.g. does the patient from sample X has a heart disease? This is done be answering a few subquestion, called separations. To define what separation to put in the root, the gini impurity is calculated for all separations. Gini impurity can be described as the measure of how often an element in a set is wrongly labeled. Therefore, a low “gini-score” is preferable. The separation with the lowest gin-score is therefore set as the root of the tree.\n",
        "\n",
        "Out of a decision tree, a random forest can be built. This is done since regular decision trees are easy to interpret, but not good when new samples are to be classified. \n",
        " First thing to do when creating a random forest is to create a bootstrap dataset, which basically consist of randomly selected samples from the original dataset. In the bootstrap dataset, samples could be used multiple times. To build the tree, two random variables from the bootstrap dataset are selected and the one with the lowest gini-score gets selected as the root. After that the same procedure goes for the following nodes, with the exception that the same column can’t be selected twice. When all variables are placed in the tree, a new random tree is created through the same procedure as described above. This is repeated as many times as possible.\n",
        "\n",
        "When a new sample is to be classified, the sample is run through all trees and the classifications from the different trees are counted. The option with the most “votes” is defined the answer.\n",
        "\n",
        "7.23\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLybEuZdDcWM",
        "outputId": "6f32ee0e-2b69-41d7-ae2f-1f0b538a473d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        }
      },
      "source": [
        "Image(url= \"https://i.postimg.cc/3x8TzMM1/leafnode.jpg\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://i.postimg.cc/3x8TzMM1/leafnode.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV12xTwioLeu"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-9wOMBTbBiS"
      },
      "source": [
        "5b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr_1iKHQbC-7"
      },
      "source": [
        " **Supervised learning** is the process of learning a function that generates an output from a certain input based on multiple similar examples of inputs and outputs.\n",
        "\n",
        "**Reinforcement learning** is another example of machine learning where the function learns to make a sequence of decisions. The optimal solutions is reached through assigning either rewards or penalties for certain actions. The goal of the function is to maximise the total reward. It is the programmer that sets the rewards and penalties. However, no hints are given regarding how to maximise the output. \n",
        "\n",
        "The biggest difference between the to learning algoritms is that supervised learning maps labeled input to known output and reinforcement learning follows a trail-and error method. The main tasks in the first-mentioned learning method is that the main task here is regression and classification. In reinforcement learning, there are multiple tasks that can be done, e.g. MDP and exploitation and exploration. Moreover, supervised learning aims to generate a formula given similar input-output-pairs, whereas reinforcement learning aims to make an algoritm that interacts with an environment by themselves. Another difference is that the data in supervised learning is labeled whereas the reinforcement learning has no predefined data. "
      ]
    }
  ]
}
